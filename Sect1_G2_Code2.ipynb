{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cac77a0",
   "metadata": {},
   "source": [
    "# Group Number 2 - Members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947b4ace",
   "metadata": {},
   "source": [
    "* Ali Annan 202475973\n",
    "* Kinan Morad 202471895\n",
    "* Sasha Nasser 202473486\n",
    "* Romanos Rizk 202471561\n",
    "* Rita Salloum 202371596"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7938a5",
   "metadata": {},
   "source": [
    "# Importing The Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ff2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import ast\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652effbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Users/Usr/Desktop/newestnewnewbalance_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af3efc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2c96ea",
   "metadata": {},
   "source": [
    "## Subsetting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1d01ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset into a DataFrame named 'data' (assuming you have already loaded it)\n",
    "\n",
    "# Define features to balance\n",
    "features_to_balance = ['section_name', 'type_of_material', 'news_desk']\n",
    "\n",
    "# Calculate value counts for each feature\n",
    "value_counts = {}\n",
    "for feature in features_to_balance:\n",
    "    value_counts[feature] = data[feature].value_counts()\n",
    "\n",
    "# Determine minimum target counts for each feature\n",
    "min_target_counts = {feature: value_counts[feature].min() for feature in features_to_balance}\n",
    "\n",
    "# Set desired sample size\n",
    "desired_sample_size = 20000\n",
    "\n",
    "# Initialize balanced dataset\n",
    "balanced_dataset = pd.DataFrame()\n",
    "\n",
    "# Iterate through features to balance\n",
    "for feature in tqdm(features_to_balance):\n",
    "    unique_values = value_counts[feature].index\n",
    "    # Iterate through unique values of the feature\n",
    "    for unique_value in unique_values:\n",
    "        if len(balanced_dataset) >= desired_sample_size:\n",
    "            break  # Stop when the desired sample size is reached\n",
    "        samples_to_select = min_target_counts[feature]\n",
    "        selected_indices = data[data[feature] == unique_value].sample(min(samples_to_select, desired_sample_size - len(balanced_dataset)), random_state=42).index\n",
    "        balanced_dataset = pd.concat([balanced_dataset, data.loc[selected_indices]])\n",
    "\n",
    "# Check if desired sample size is not reached and oversample\n",
    "remaining_samples = desired_sample_size - len(balanced_dataset)\n",
    "if remaining_samples > 0:\n",
    "    oversample_indices = data.sample(remaining_samples).index\n",
    "    balanced_dataset = pd.concat([balanced_dataset, data.loc[oversample_indices]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6328a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = balanced_dataset\n",
    "del balanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1d90871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39dee97",
   "metadata": {},
   "source": [
    "# Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8460a9d",
   "metadata": {},
   "source": [
    "## Removing Non Relevant COlumns and Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8548fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between 'word_count' and 'section_name' (Cramér's V): 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Assuming you have a DataFrame 'df' with columns 'word_count' (numerical) and 'section_name' (categorical)\n",
    "# You may need to preprocess your data and encode categorical variables before performing correlation analysis\n",
    "\n",
    "# Compute Cramér's V for 'word_count' and 'section_name'\n",
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "    rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
    "    kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
    "    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
    "\n",
    "# Create a cross-tabulation of 'word_count' and 'section_name'\n",
    "confusion_matrix = pd.crosstab(data['word_count'], data['section_name'])\n",
    "\n",
    "# Compute Cramér's V\n",
    "correlation = cramers_v(confusion_matrix)\n",
    "\n",
    "print(\"Correlation between 'word_count' and 'section_name' (Cramér's V):\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09836ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your DataFrame\n",
    "columns_to_drop = ['web_url', 'print_section', 'print_page', 'source', 'multimedia', \n",
    "                   'document_type', 'news_desk', 'byline', 'type_of_material', '_id', 'uri', 'word_count']\n",
    "data = data.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1184a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the allowed sections\n",
    "allowed_sections = ['Arts', 'Automobiles', 'Blogs', 'Books', 'Business Day', 'College', 'Climate', \n",
    "                    'Education', 'Fashion & Style', 'Food', 'Health', 'Home & Garden', 'Job Market', \n",
    "                    'Movies', 'Parenting', 'Podcasts', 'Real Estate', 'Science', 'Sports', 'Technology', \n",
    "                    'Theater', 'Travel', 'U.S.', 'World']\n",
    "\n",
    "# Filtering the rows based on allowed sections\n",
    "data = data[data['section_name'].isin(allowed_sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2099706e",
   "metadata": {},
   "source": [
    "## Dropping Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f62d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns='headline', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d586d97",
   "metadata": {},
   "source": [
    "## Extracting the Headline of each Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8eb0976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24656    {'main': 'Sprint and SK Telecom Said to Discus...\n",
      "42082    {'main': '‘It’s a Weird Feeling’: Seattle Hunk...\n",
      "45004    {'main': 'With Bombing, Iraqis Escalate Guerri...\n",
      "14331    {'main': 'U.S. Team Loses More Players for Qua...\n",
      "14535    {'main': 'Displaying the Discreet Charm and Ca...\n",
      "Name: headline, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the first few raw entries of the 'headline' column\n",
    "print(data['headline'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "463f8af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24656    Sprint and SK Telecom Said to Discuss Partnership\n",
      "42082    ‘It’s a Weird Feeling’: Seattle Hunkers Down A...\n",
      "45004    With Bombing, Iraqis Escalate Guerrilla Tactic...\n",
      "14331          U.S. Team Loses More Players for Qualifiers\n",
      "14535    Displaying the Discreet Charm and Casual Grace...\n",
      "Name: main_headline, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert string representations of dictionaries to actual dictionaries\n",
    "data['headline'] = data['headline'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Now, extract the 'main' key from each dictionary in the 'headline' column\n",
    "data['main_headline'] = data['headline'].apply(lambda x: x['main'] if 'main' in x else None)\n",
    "\n",
    "# Check the first few entries of the main_headline to confirm success\n",
    "print(data['main_headline'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7274271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24656    Sprint and SK Telecom Said to Discuss Partners...\n",
      "42082    ‘It’s a Weird Feeling’: Seattle Hunkers Down A...\n",
      "45004    With Bombing, Iraqis Escalate Guerrilla Tactic...\n",
      "14331    U.S. Team Loses More Players for Qualifiers Th...\n",
      "14535    Displaying the Discreet Charm and Casual Grace...\n",
      "Name: romanos, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Combine the columns into one\n",
    "data['romanos'] = data['main_headline'].astype(str) + \" \" + data['snippet'].astype(str) + \"  \" + data['abstract'].astype(str) + \" \" + data['lead_paragraph'].astype(str)\n",
    "\n",
    "# Check the first few entries to ensure it's combined correctly\n",
    "print(data['romanos'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150f4cc",
   "metadata": {},
   "source": [
    "## Dropping Non Relevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a785fee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                keywords  \\\n",
      "24656                                                 []   \n",
      "42082  [{'name': 'subject', 'value': 'Coronavirus (20...   \n",
      "45004  [{'name': 'glocations', 'value': 'Iraq', 'rank...   \n",
      "14331  [{'name': 'persons', 'value': 'Castillo, Edgar...   \n",
      "14535  [{'name': 'persons', 'value': 'Louis, Murray',...   \n",
      "\n",
      "                        pub_date  section_name subsection_name  \\\n",
      "24656  2008-07-16 11:43:29+00:00  Business Day             NaN   \n",
      "42082  2020-03-06 02:33:26+00:00          U.S.             NaN   \n",
      "45004  2003-03-30 05:00:00+00:00         World             NaN   \n",
      "14331  2012-10-10 23:57:43+00:00        Sports          Soccer   \n",
      "14535  2007-09-06 04:00:00+00:00          Arts           Dance   \n",
      "\n",
      "                                                 romanos  \n",
      "24656  Sprint and SK Telecom Said to Discuss Partners...  \n",
      "42082  ‘It’s a Weird Feeling’: Seattle Hunkers Down A...  \n",
      "45004  With Bombing, Iraqis Escalate Guerrilla Tactic...  \n",
      "14331  U.S. Team Loses More Players for Qualifiers Th...  \n",
      "14535  Displaying the Discreet Charm and Casual Grace...  \n"
     ]
    }
   ],
   "source": [
    "# Drop the specified columns\n",
    "data.drop(columns=['snippet', 'lead_paragraph', 'main_headline', 'abstract'], inplace=True)\n",
    "\n",
    "# Check the DataFrame to confirm that the columns have been removed\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad05f725",
   "metadata": {},
   "source": [
    "## Extracting the Keywords of each Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ba2b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to safely convert string representations to actual lists\n",
    "def convert_to_list(keyword_string):\n",
    "    try:\n",
    "        return ast.literal_eval(keyword_string) if isinstance(keyword_string, str) else keyword_string\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None  # Return None if the string cannot be converted\n",
    "\n",
    "# Apply this conversion function to the entire 'keywords' column\n",
    "data['keywords'] = data['keywords'].apply(convert_to_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebc37143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24656                                                 None\n",
      "42082    subject: Coronavirus (2019-nCoV), subject: Tel...\n",
      "45004    glocations: Iraq, glocations: Najaf (Iraq), gl...\n",
      "14331    persons: Castillo, Edgar, persons: Donovan, La...\n",
      "14535    persons: Louis, Murray, organizations: Common ...\n",
      "Name: keyword_sentences, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def format_keywords(keywords):\n",
    "    # Ensure that keywords is a list and not None\n",
    "    if isinstance(keywords, list) and keywords:\n",
    "        # Extract 'name' and 'value' and format into a string, excluding 'rank' and 'major'\n",
    "        keyword_strings = [f\"{keyword['name']}: {keyword['value']}\" for keyword in keywords if 'name' in keyword and 'value' in keyword]\n",
    "        # Join all strings into a single sentence\n",
    "        return ', '.join(keyword_strings)\n",
    "    return None\n",
    "\n",
    "# Apply the formatting function to the 'keywords' column again\n",
    "data['keyword_sentences'] = data['keywords'].apply(format_keywords)\n",
    "\n",
    "# Check the results\n",
    "print(data['keyword_sentences'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f6053",
   "metadata": {},
   "source": [
    "## Dropping Non Relevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5fcf053",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns= 'keywords', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcfc5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = 'pub_date', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a474f7",
   "metadata": {},
   "source": [
    "## Combining all the Text into a 'combined_text' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d851fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          section_name                                      combined_text\n",
      "14331           Sports  U.S. Team Loses More Players for Qualifiers Th...\n",
      "14535             Arts  Displaying the Discreet Charm and Casual Grace...\n",
      "1694   Fashion & Style  Mitch McEwen, Dina Paulson The couple are to b...\n",
      "23472             Food  Cocktail School Lets Novices Be the Bartender ...\n",
      "15576         Podcasts  Why Are All Eyes on the Virginia Governor’s Ra...\n"
     ]
    }
   ],
   "source": [
    "# Combine the text from 'romanos' and 'keyword_sentences' into a new column 'combined_text'\n",
    "data['combined_text'] = data['romanos'] + \" \" + data['keyword_sentences'] + \" \" + data['subsection_name']\n",
    "\n",
    "# Drop the original columns if needed\n",
    "data.drop(['romanos', 'keyword_sentences', 'subsection_name'], axis=1, inplace=True)\n",
    "\n",
    "# Print the DataFrame to verify the changes\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f82c73",
   "metadata": {},
   "source": [
    "## Converting Article Text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3473786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          section_name                                      combined_text\n",
      "14331           Sports  u.s. team loses more players for qualifiers th...\n",
      "14535             Arts  displaying the discreet charm and casual grace...\n",
      "1694   Fashion & Style  mitch mcewen, dina paulson the couple are to b...\n",
      "23472             Food  cocktail school lets novices be the bartender ...\n",
      "15576         Podcasts  why are all eyes on the virginia governor’s ra...\n"
     ]
    }
   ],
   "source": [
    "# Define a function for preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to each text column\n",
    "data['combined_text'] = data['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Print the first few rows to verify the result\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11c030c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b62ac9",
   "metadata": {},
   "source": [
    "# LLM Model - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db4b4e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Usr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Usr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usr\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Usr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8863361547762999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize text and convert to numerical format\n",
    "def tokenize_and_encode(text, max_length):\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'  # Return PyTorch tensors\n",
    "    )\n",
    "    return encoded_text['input_ids'], encoded_text['attention_mask']\n",
    "\n",
    "# Define BERT model\n",
    "num_labels = len(data['section_name'].unique())\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=num_labels,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label2id = {label: i for i, label in enumerate(data['section_name'].unique())}\n",
    "train_data['label'] = train_data['section_name'].map(label2id)\n",
    "test_data['label'] = test_data['section_name'].map(label2id)\n",
    "\n",
    "# Tokenize and encode train and test data\n",
    "max_length = 128  # Maximum sequence length\n",
    "train_input_ids, train_attention_mask = zip(*train_data['combined_text'].apply(lambda x: tokenize_and_encode(x, max_length)))\n",
    "test_input_ids, test_attention_mask = zip(*test_data['combined_text'].apply(lambda x: tokenize_and_encode(x, max_length)))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_input_ids = torch.cat(train_input_ids, dim=0)\n",
    "train_attention_mask = torch.cat(train_attention_mask, dim=0)\n",
    "train_labels = torch.tensor(train_data['label'].values)\n",
    "\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_mask = torch.cat(test_attention_mask, dim=0)\n",
    "test_labels = torch.tensor(test_data['label'].values)\n",
    "\n",
    "# Create DataLoader for train and test sets\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Train BERT model\n",
    "epochs = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = [t.to(device) for t in batch]\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "# Evaluate BERT model\n",
    "model.eval()\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress_bar = tqdm(test_dataloader, desc='Evaluation', leave=False)\n",
    "    for batch in progress_bar:\n",
    "        input_ids, attention_mask, labels = [t.to(device) for t in batch]\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predicted_labels.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
